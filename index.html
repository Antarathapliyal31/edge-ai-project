<!DOCTYPE html>
<html>
<head>
    <title>Edge AI Chat</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 20px;
        }
        #response {
            border: 1px solid #ccc;
            padding: 10px;
            width: 80%;
            min-height: 100px;
            white-space: pre-wrap;
        }    
    </style>
</head>
<body>
    <h1>Browser LLM Test</h1>
    <div id="status">Loading model...</div>
    <br>
    <textarea id="prompt" rows="3" cols="50" placeholder="Type your message here..."></textarea>
    <br>
    <button id="send" disabled>Send</button>
    <br><br>
    <div id="response"></div>
    <br>
    <div id="metrics" style="background-color: #f0f0f0; padding: 10px; font-family: monospace;"></div>

    <script type="module">
        import * as webllm from "https://esm.run/@mlc-ai/web-llm";

        const statusEl = document.getElementById("status");
        const promptEl = document.getElementById("prompt");
        const sendBtn = document.getElementById("send");
        const responseEl = document.getElementById("response");
        const metricsEl = document.getElementById("metrics");

        const engine = new webllm.MLCEngine();

        async function initModel() {
            const modelLoadStart = performance.now();
            statusEl.textContent = "Downloading model... (this takes a few minutes the first time)";

            await engine.reload("Llama-3.1-8B-Instruct-q4f32_1-MLC", {
                initProgressCallback: (progress) => {
                    statusEl.textContent = progress.text;
                }
            });

            const modelLoadEnd = performance.now();
            const modelLoadTime = ((modelLoadEnd - modelLoadStart) / 1000).toFixed(1);

            statusEl.textContent = "Model ready! (loaded in " + modelLoadTime + " seconds)";
            sendBtn.disabled = false;
        }

        sendBtn.addEventListener("click", async () => {
            const userMessage = promptEl.value;
            if (!userMessage) return;

            sendBtn.disabled = true;
            responseEl.textContent = "";
            metricsEl.textContent = "Generating...";

            const startTime = performance.now();
            let firstTokenTime = null;
            let tokenCount = 0;

            const reply = await engine.chat.completions.create({
                messages: [{ role: "user", content: userMessage }],
                temperature: 0.0,
                max_tokens: 256,
                stream: true,
            });

            for await (const chunk of reply) {
                const token = chunk.choices[0]?.delta?.content || "";
                if (token && !firstTokenTime) {
                    firstTokenTime = performance.now();
                }
                if (token) {
                    tokenCount++;
                    responseEl.textContent += token;
                }
            }

            const endTime = performance.now();

            const ttft = firstTokenTime - startTime;
            const totalTime = endTime - startTime;
            const decodeTime = endTime - firstTokenTime;
            const tokensPerSec = tokenCount / (decodeTime / 1000);

            metricsEl.innerHTML = 
                "<b>Performance Metrics:</b><br>" +
                "Time to First Token (TTFT): " + ttft.toFixed(0) + " ms<br>" +
                "Tokens Per Second (TPS): " + tokensPerSec.toFixed(1) + " tokens/sec<br>" +
                "Total Tokens Generated: " + tokenCount + "<br>" +
                "Total Response Time: " + (totalTime / 1000).toFixed(2) + " seconds<br>" +
                "Decode Time: " + (decodeTime / 1000).toFixed(2) + " seconds";

            sendBtn.disabled = false;
        });

        initModel();
    </script>
</body>
</html>
