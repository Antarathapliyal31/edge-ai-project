<!DOCTYPE html>
<html>
<head>
    <title>Edge AI Chat</title>
</head>
<body>
    <h1>Browser LLM Test</h1>
    <div id="status">Loading model...</div>
    <br>
    <textarea id="prompt" rows="3" cols="50" placeholder="Type your message here..."></textarea>
    <br>
    <button id="send" disabled>Send</button>
    <br><br>
    <div id="response"></div>

    <script type="module">
        import * as webllm from "https://esm.run/@mlc-ai/web-llm";

        const statusEl = document.getElementById("status");
        const promptEl = document.getElementById("prompt");
        const sendBtn = document.getElementById("send");
        const responseEl = document.getElementById("response");

       
        const engine = new webllm.MLCEngine();

        async function initModel() {
            statusEl.textContent = "Downloading model... (this takes a few minutes the first time)";

            await engine.reload("Llama-3.1-8B-Instruct-q4f32_1-MLC", {
                initProgressCallback: (progress) => {
                    statusEl.textContent = progress.text;
                }
            });

            statusEl.textContent = "Model ready!";
            sendBtn.disabled = false;
        }

       sendBtn.addEventListener("click", async () => {
            const userMessage = promptEl.value;
            if (!userMessage) return;

            sendBtn.disabled = true;
            responseEl.textContent = "";

            const startTime = performance.now();
            let firstTokenTime = null;
            let tokenCount = 0;

            const reply = await engine.chat.completions.create({
                messages: [{ role: "user", content: userMessage }],
                temperature: 0.2,
                stream: true,
            });

            for await (const chunk of reply) {
                const token = chunk.choices[0]?.delta?.content || "";
                if (token && !firstTokenTime) {
                    firstTokenTime = performance.now();
                }
                if (token) {
                    tokenCount++;
                    responseEl.textContent += token;
                }
            }

            const endTime = performance.now();

            // Calculate metrics
            const ttft = firstTokenTime - startTime;           // Time to First Token (ms)
            const totalTime = endTime - startTime;              // Total generation time (ms)
            const decodeTime = endTime - firstTokenTime;        // Time spent generating after first token
            const tokensPerSec = tokenCount / (decodeTime / 1000);  // Tokens per second

            console.log(`TTFT: ${ttft.toFixed(0)}ms`);
            console.log(`Total time: ${totalTime.toFixed(0)}ms`);
            console.log(`Tokens: ${tokenCount}`);
            console.log(`Speed: ${tokensPerSec.toFixed(1)} tokens/sec`);

            sendBtn.disabled = false;
});

        initModel();
    </script>
</body>
</html>